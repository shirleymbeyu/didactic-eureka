# -*- coding: utf-8 -*-
"""Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hcncz_TNsGA7Vscel65Sh3Y2ae9qPzFP

#Set up
"""

!pip install --upgrade fastcore -q
!pip install --upgrade fastai -q

from fastai.vision.all import * # Needs latest version, and sometimes a restart of the runtime after the pip installs

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
np.random.seed(123)
warnings.filterwarnings('ignore')
# %matplotlib inline

train = pd.read_csv('train.csv')
train.head()

train.shape

test = pd.read_csv('test.csv')
test.head()

"""# Data Preparation"""

#import preprocessing module
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler

# Convert target label to numerical Data
le = LabelEncoder()
train["bank_accnt"]= le.fit_transform(train["bank_accnt"])

sns.countplot('bank_accnt', data = train);

X_train = train.drop(['bank_accnt'], axis=1)
y_train = train['bank_accnt']

"""We have created a simple preprocessing function to:

Handle conversion of data types

Convert categorical features to numerical features by using One-hot Encoder and Label Encoder

Perform feature scaling.

The processing function will be used for both train and test independent variables.
"""

# function to preprocess our data from train models

def preprocessing_data(data):

    # Convert the following numerical labels from interger to float
    float_array = data[["houseSize", "age", "year"]].values.astype(float)
    
    # categorical features to be onverted to One Hot Encoding
    categ = [
        "rshp_w_hd",
        "status",
        "job",
        "country",
        "education"
    ]
    
    # One Hot Encoding conversion
    data = pd.get_dummies(data, prefix_sep="_", columns=categ)
    
    # Label Encoder conversion
    data["location"] = le.fit_transform(data["location"])
    data["cellphone"] = le.fit_transform(data["cellphone"])
    data["gender"] = le.fit_transform(data["gender"])
    
    # drop uniquid column
    data = data.drop(["unique_id"], axis=1)
    
    # scale our data into range of 0 and 1
    scaler = MinMaxScaler(feature_range=(0, 1))
    data = scaler.fit_transform(data)
    
    return data

# preprocess the train data 
processed_train_data = preprocessing_data(X_train)
processed_test_data = preprocessing_data(test)

# the first train row
print(processed_train_data[:1])

# shape of the processed train set
print(processed_train_data.shape)

"""#Modelling"""

!pip install xgboost==0.90

from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(processed_train_data, y_train, test_size = 0.1, random_state=42, stratify = y_train)

#import classifier algorithm here
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import ExtraTreesClassifier
from xgboost import XGBClassifier


# create models
lg_model = LogisticRegression()
rf_model = RandomForestClassifier()
kn_model = KNeighborsClassifier()
et_model = ExtraTreesClassifier()
xg_model = XGBClassifier()


#training the models
lg_model.fit(X_train,y_train)
rf_model.fit(X_train,y_train)
kn_model.fit(X_train,y_train)
et_model.fit(X_train,y_train)
xg_model.fit(X_train,y_train)

#making predictions
lg_y_pred = lg_model.predict(X_val)
rf_y_pred = rf_model.predict(X_val)
kn_y_pred = kn_model.predict(X_val)
et_y_pred = et_model.predict(X_val)
xg_y_pred = xg_model.predict(X_val)

"""## Model Evaluation

The evaluation metric for this challenge will be the percentage of survey respondents for whom you predict the binary 'bank account' classification incorrectly.

This means the lower the incorrect percentage we get, the better the model performance.
"""

# import evaluation metrics
from sklearn.metrics import confusion_matrix, accuracy_score

# evaluate the model
# Get error rate
print("Error rate Logistic Regression classifier: ", 1 - accuracy_score(y_val, lg_y_pred))
print("Error rate of Random Forest classifier: ", 1 - accuracy_score(y_val, rf_y_pred))
print("Error rate of KNeighbors Classifier: ", 1 - accuracy_score(y_val, kn_y_pred))
print("Error rate of Extra Tree classifier: ", 1 - accuracy_score(y_val, et_y_pred))
print("Error rate of XGB classifier: ", 1 - accuracy_score(y_val, xg_y_pred))

from sklearn.metrics import plot_confusion_matrix

# Get confusion matrix for Gradient Boosting Classifier 
plot_confusion_matrix( xg_model,X_val, y_val,normalize='true')

"""Our XGBoost model performs well on predicting class 0 and performs poorly on predicting class 1.

It may be caused by the imbalance of data provided(the target variable has more ‘No’ values than ‘Yes’ values). You can learn the best way to deal with imbalanced data here.

## Increasing Model Perfomance

One way to increase the model performance is by applying the Grid search method as an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.
"""

from sklearn.model_selection import GridSearchCV

# Optimize model paramaters 
param_grid = {'min_child_weight': [1, 5, 10],
        'gamma': [0.5, 1],
        'subsample': [0.6, 0.8, 1.0],
        'max_depth': [3,5]
        }
my_xg_model = GridSearchCV(xg_model, param_grid,n_jobs=-1,verbose=2,cv=5)
my_xg_model.fit(X_train, y_train)
print(my_xg_model.best_params_)

from sklearn.metrics import confusion_matrix, accuracy_score


# fit by setting best parameters and Evaluate model
xgb_model = XGBClassifier(min_child_weight=1, gamma=1, subsample=0.8, max_depth=5)

xgb_model.fit(X_train, y_train)
y_pred = xgb_model.predict(X_val)

# Get error rate
print("Error rate of the  XGB classifier: ", 1 - accuracy_score(y_val, y_pred))

"""# Predicting"""

# Get the predicted result for the test Data
test_bank_account = xgb_model.predict(processed_test_data)

test_bank_account.shape

"""# Submission"""

# crete submission DataFrame
submission = pd.DataFrame({"uniqueid": test["unique_id"],"bank_account": test_bank_account})

submission

submission[submission['uniqueid'] == 'uniqueid_7867 x Kenya']

submission.to_csv('ss4.csv', index= False)

ss1 = pd.read_csv('ss3.csv')
ss1