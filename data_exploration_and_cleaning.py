# -*- coding: utf-8 -*-
"""Data Exploration and Cleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1USXGAY9fJZR3umO56C5NFv1FfrE6PtUU

# Set_up
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
np.random.seed(123)
warnings.filterwarnings('ignore')
# %matplotlib inline

train = pd.read_csv('Train_v2.csv')
train.head()

test = pd.read_csv('Test_v2.csv')
test.head()

var = pd.read_csv('VariableDefinitions.csv')
var

ss = pd.read_csv('SubmissionFile.csv')
ss.head()

"""# Data Exploration"""

train.shape

train.info()

#unique elements in our data:
cols = train.columns.to_list()

for col in cols:
  print('COLUMN:', col)
  print('Number of unique variables:', train[col].nunique())
  print(train[col].unique())
  print()

#checking for missing values:
train.isna().sum()

#checking for duplicates
train.duplicated().any()

"""# Data Cleaning"""

#merge the country and id columns to match those in the submission file.
train[id] = train['uniqueid'] + ' ' +"x" + ' '+ train['country']
test[id] = test['uniqueid'] + ' ' +"x" + ' '+ test['country']

#changing column names
train.columns = ['country', 'year', 'id', 'bank_accnt','location', 'cellphone', 'houseSize', 'age', 'gender', 'rshp_w_hd', 'status', 'education', 'job', 'unique_id']
test.columns = ['country', 'year', 'id','location', 'cellphone', 'houseSize', 'age', 'gender', 'rshp_w_hd', 'status', 'education', 'job', 'unique_id']

train = train.drop(columns = ['id'])
test = test.drop(columns = ['id'])

test.shape

"""Confirming the change:"""

train.sample()

test.sample()

test.isna().sum()

#exporting our cleaned data to csv for use in making the model
train.to_csv('train', index= False)
test.to_csv('test', index = False)